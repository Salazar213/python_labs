import string
def normalize (text1: str, casefold: bool = True , yo2e:bool = True)-> str:
    if type(text1) != str:
        raise TypeError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö—Ö")
    if yo2e:
        text1 = text1.replace('—ë','–µ')
        text1 = text1.replace("–Å","–ï")
    
    if casefold:
        text1 = text1.casefold()

    text1 = ' '.join(text1.split())
    text1 = ' '.join(text1.split('\t'))
    text1 = ' '.join(text1.split('\r'))
    text1 = ' '.join(text1.split('\n'))

    return text1
def tokenize(text2: str) -> list[str]:
    
    alph1 = ',./~!@#$%^&*()<>}{=+!"‚Ññ;%:?*()‚Äî'
    alph2 = "'"
    alph3 = '_-'
    alph_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è0123456789'
    for i in alph1:
        text2 = text2.replace(i, ' ')
    for j in alph2:
        text2 = text2.replace(j, ' ')
    text2 = [_ for _ in text2]
    for i in range(len(text2)):
        try:
            if (text2[i] in alph3) and (text2[i-1] in alph_letters and text2[i+1] in alph_letters):
                pass
            else:
                if text2[i] in alph3:
                    text2[i] = ' '
        except:
            if text2[i] in alph3:
                text2[i] = ' '
    text2 = ''.join(text2)
    text2 = text2.split()
    i = 0
    for element in text2:
        
        for letter in element:
            if not(letter in alph_letters) and letter not in alph3:
                element = element.replace(letter,'')
                text2[i] = element
        i+=1
    text2 = [i for i in text2 if i!=""]
    return text2


def count_freq(tokens: list[str]) -> dict[str, int]:
    ans = dict()
    if type(tokens) != list:
        raise TypeError(f"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å list, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(tokens)}")
    try:

        if type(tokens[0])!=str:
            raise TypeError(f'"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å list[str], –ø–µ—Ä–µ–¥–∞–Ω–æ list[{type(tokens[0])}]"')
    except:
        pass

    try:
        type_tokens = set(list(map(type, tokens)))
        if len(set)!=1:
            raise TypeError(f"–í–Ω—É—Ç—Ä–∏ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö {type_tokens}")
    except:
        pass
    for element in tokens:
        if element not in ans:
            ans.update({element : tokens.count(element)})
    return ans


def top_n(freq: dict[str, int], n: int = 5)-> list[tuple[str, int]]:
    if  type(freq) != dict:
        raise TypeError(f'–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å dict, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(freq)}')
    if  type(n) != int:
        raise TypeError(f'–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å int, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(n)}')
    freq = sorted(freq.items(),key=lambda x: (-x[1],x[0]))
    if n >len(freq):
        return freq
    else:
        return freq[:n]


def count_freq_top(tokens: list[str]) -> dict[str, int]:
    tokens = list(input().split())
    ans = dict()
    for element in tokens:
        if element not in ans:
            ans.update({element : tokens.count(element)})
    return(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}")

# a = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"
# b = "—ë–∂–∏–∫, –Å–ª–∫–∞"
# c = "Hello\r\nWorld"
# d = "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "

# print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(c)}\n–°—Ç—Ä–æ–∫–∞:\n{d}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(d)} ')
# a = "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
# b = "hello,world!!!"
# c = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"
# d = "2025 –≥–æ–¥"
# e = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"

# print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(c)}\n–°—Ç—Ä–æ–∫–∞:\n{d}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(d)}\n–°—Ç—Ä–æ–∫–∞:\n{e}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{tokenize(e)} ')
a = ""
b = "üòÄüòÄüòÄ.ha ha-haüòÄüòÄüòÄüòÄüòÄ"
c = "–í —Ç–∞–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: 2020-2025!!!!"
print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(c)}')
