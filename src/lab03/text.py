import string


def normalize(text1: str, casefold: bool = True, yo2e: bool = True) -> str:
    if type(text1) != str:
        raise TypeError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö—Ö")
    if yo2e:
        text1 = text1.replace("—ë", "–µ")
        text1 = text1.replace("–Å", "–ï")

    if casefold:
        text1 = text1.casefold()

    text1 = " ".join(text1.split())
    text1 = " ".join(text1.split("\t"))
    text1 = " ".join(text1.split("\r"))
    text1 = " ".join(text1.split("\n"))

    return text1


def tokenize(text2: str) -> list[str]:

    alph1 = ',./~!@#$%^&*()<>}{=+!"‚Ññ;%:?*()‚Äî'
    alph2 = "'"
    alph3 = "_-"
    alph_letters = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è0123456789"
    for i in alph1:
        text2 = text2.replace(i, " ")
    for j in alph2:
        text2 = text2.replace(j, " ")
    text2 = [_ for _ in text2]
    for i in range(len(text2)):
        try:
            if (text2[i] in alph3) and (
                text2[i - 1] in alph_letters and text2[i + 1] in alph_letters
            ):
                pass
            else:
                if text2[i] in alph3:
                    text2[i] = " "
        except:
            if text2[i] in alph3:
                text2[i] = " "
    text2 = "".join(text2)
    text2 = text2.split()
    i = 0
    for element in text2:

        for letter in element:
            if not (letter in alph_letters) and letter not in alph3:
                element = element.replace(letter, "")
                text2[i] = element
        i += 1
    text2 = [i for i in text2 if i != ""]
    return text2


def count_freq(tokens: list[str]) -> dict[str, int]:
    ans = dict()
    if type(tokens) != list:
        raise TypeError(
            f"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å list, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(tokens)}"
        )
    try:

        if type(tokens[0]) != str:
            raise TypeError(
                f'"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å list[str], –ø–µ—Ä–µ–¥–∞–Ω–æ list[{type(tokens[0])}]"'
            )
    except:
        pass

    try:
        type_tokens = set(list(map(type, tokens)))
        if len(set) != 1:
            raise TypeError(f"–í–Ω—É—Ç—Ä–∏ —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö {type_tokens}")
    except:
        pass
    for element in tokens:
        if element not in ans:
            ans.update({element: tokens.count(element)})
    return ans


def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    if type(freq) != dict:
        raise TypeError(f"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å dict, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(freq)}")
    if type(n) != int:
        raise TypeError(f"–ù–µ –≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å int, –ø–µ—Ä–µ–¥–∞–Ω–æ {type(n)}")
    freq = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
    if n > len(freq):
        return freq
    else:
        return freq[:n]


def count_freq_top(tokens: list[str]) -> dict[str, int]:
    tokens = list(input().split())
    ans = dict()
    for element in tokens:
        if element not in ans:
            ans.update({element: tokens.count(element)})
    return f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}"


# a = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"
# b = "—ë–∂–∏–∫, –Å–ª–∫–∞"
# c = "Hello\r\nWorld"
# d = "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "

# print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(c)}\n–°—Ç—Ä–æ–∫–∞:\n{d}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{normalize(d)} ')
# a = "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
# b = "hello,world!!!"
# c = "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"
# d = "2025 –≥–æ–¥"
# e = "emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"

# print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(c)}\n–°—Ç—Ä–æ–∫–∞:\n{d}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(d)}\n–°—Ç—Ä–æ–∫–∞:\n{e}\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞:\n{tokenize(e)} ')
# a = ""
# b = "üòÄüòÄüòÄ.ha ha-haüòÄüòÄüòÄüòÄüòÄ"
# c = "–í —Ç–∞–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ: 2020-2025!!!!"
# print(f'–°—Ç—Ä–æ–∫–∞:\n{a}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(a)}\n–°—Ç—Ä–æ–∫–∞:\n{b}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(b)}\n–°—Ç—Ä–æ–∫–∞:\n{c}\n–û—Ç–¥–µ–ª—å–Ω–æ —Å–ª–æ–≤–∞:\n{tokenize(c)}')
